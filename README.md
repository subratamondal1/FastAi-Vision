# FASTAI COMPUTER VISION

# P1: üê∂üê±Cat & Dog breed detectorüêïüêà

## Project Description
In this deep learning project, we have developed a sophisticated Cat & Dog breed detector using the state-of-the-art Fastai framework. Our goal was to accurately classify pet images into 37 distinct categories, representing various breeds. We utilized the challenging Oxford-IIIT Pet Dataset, which contains a diverse set of pet images with complex variations in scale, pose, and lighting.

## FastAi Computer Vision Pipeline for Cat & Dog breed detector

### Load Data 
- The project focuses on the [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/), known for its 37 distinct pet categories and high-quality images.
- Essential libraries, including FastAi, are imported to support the computer vision project.
- The FastAi version used in this code is 2.7.12.
- The FastAi vision package is imported to leverage its modules and functions for computer vision tasks.
- The `get_image_files` function is imported to facilitate the retrieval of image file paths.
- The Oxford-IIIT Pet Dataset, accessible through the provided URL, is downloaded and untarred for data preparation.
- A list of image file names residing in the 'images' directory of the dataset is obtained using the `get_image_files` function.
- The code prints the total count of images in the dataset, which is 7390 images.
- Additionally, it displays the file paths of the first 10 images in the dataset, providing a glimpse of the data's structure and location.

### Data Preparation
- **Statistical Normalization**: The code imports essential statistics from the FastAi library, specifically, the `imagenet_stats`. These statistics are widely utilized for image data normalization, a fundamental step in preparing data for deep learning models.

- **Image Augmentation and Cropping**: The code incorporates critical image transformation functions from FastAi, such as `aug_transforms` and `RandomResizedCrop`. These transformations are pivotal for introducing variability and robustness into the dataset, enhancing the model's ability to generalize from the training data to unseen examples.

- **Item Transforms**: The `item_tfms` variable is defined to specify item-level transformations. In this case, it involves a random resized crop operation with dimensions of 460 pixels. This transformation not only resizes images but also introduces slight variability in scale, enhancing the dataset's diversity.

- **Batch Transforms**: A list of batch-level transformations, denoted by the `batch_tfms` variable, is defined. This list encapsulates a combination of augmentations, such as resizing to 224 pixels and maximum warping, along with data normalization using the previously imported `imagenet_stats`. These batch transforms collectively prepare the data for efficient training while maintaining statistical consistency.

By executing these meticulously designed data preparation steps, the code ensures that the dataset is suitably conditioned for subsequent phases of the computer vision pipeline. This proactive approach to data preprocessing lays the groundwork for model training, validation, and evaluation, ultimately contributing to the success of the computer vision project.

### Create DataLoader
- **DataBlock Definition**: The core of the section lies in the creation of a DataBlock named 'pets.' This DataBlock orchestrates the entire data processing pipeline, defining key aspects such as the data blocks (Image and Category) to be employed, the method for acquiring image files, data splitting into training and validation sets using random splitting, and the extraction of category labels from file names using regular expressions.

- **Item-Level and Batch-Level Transformations**: The DataBlock is configured with item-level and batch-level transformations, denoted as `item_tfms` and `batch_tfms`, respectively. These transformations, previously defined, ensure that each image and batch of data undergoes the prescribed operations, including resizing, cropping, augmentations, and normalization. This enhances the dataset's suitability for model training while maintaining consistency.

- **Data Loaders Creation**: The code concludes by creating data loaders, designated as 'dls,' utilizing the 'pets' DataBlock. These data loaders are responsible for efficiently loading and batching the data, readying it for training and validation. Specifically, the training dataset, as represented by `dls.train_ds`, contains 5912 images, while the validation dataset, as represented by `dls.valid_ds`, contains 1478 images. A batch size of 64 is specified for the data loaders.

- **Data Set Overview**: The code offers a glimpse into the training and validation datasets by printing a sample of their elements. Each element consists of a PIL image and its corresponding category label, indicative of the transformed and labeled data's readiness for model ingestion.
- **Class Vocabulary:** The code proceeds to print the number of classes within the dataset's vocabulary. In this specific dataset, there are a total of `37 distinct classes`, each corresponding to a specific pet breed. The classes are listed comprehensively, encompassing a diverse array of breeds such as Abyssinian, Bengal, Siamese, and many others.

By executing these meticulously defined data processing steps, the "Create DataLoader" section ensures that the dataset is properly structured, labeled, and preprocessed for utilization in subsequent model development phases. The resulting data loaders stand as the conduit through which the model will access and learn from the prepared data.

### Define Learner (Model) & Learning Rate
- **Mixed-Precision Training**: The code commences by enabling mixed-precision training using FastAi's `to_fp16()` method. This technique optimizes model training by using lower-precision data types, thus accelerating the training process while conserving memory.

- **Model Architecture**: A vision learner is instantiated using FastAi's `vision_learner()`. The learner is configured with key parameters, including the data loaders (`dls`) for training and validation, a ResNet-50 architecture (`arch=resnet50`), and the utilization of a pre-trained model (`pretrained=True`). The choice of ResNet-50, a deep convolutional neural network, is noteworthy for its effectiveness in image classification tasks.

- **Evaluation Metrics**: The learner is equipped with evaluation metrics, including accuracy and error rate, which will be used to assess the model's performance during training and validation.

- **Learning Rate Finder**: To determine the optimal learning rate for training the model, the code executes `learn.lr_find()`. This function employs a learning rate finder technique to identify an appropriate learning rate range for efficient convergence.

- **Optimal Learning Rate**: The code reports the discovered learning rate as a `slice(0.0001, 0.01, None)`. This represents a dynamic learning rate range that allows the model to adapt during training, starting from a smaller value and gradually increasing as the training progresses.

By executing these pivotal steps, the "Define Learner(Model) & Learning Rate" section establishes the foundational elements of our computer vision model, fine-tuning the model architecture and defining the learning rate range. These choices are critical for successful model training, ensuring both efficiency and effectiveness in the pursuit of accurate image classification.

### Train & Save Model
| epoch | train_loss | valid_loss | accuracy | error_rate |  time  |
|-------|------------|------------|----------|------------|--------|
|   0   |  0.666741  |  0.321287  | 0.895129 |  0.104871  | 01:27  |
|   1   |  0.496385  |  0.430292  | 0.875507 |  0.124493  | 01:26  |
|   2   |  0.483526  |  0.561120  | 0.868065 |  0.131935  | 01:27  |
|   3   |  0.375414  |  0.347090  | 0.908660 |  0.091340  | 01:28  |
|   4   |  0.289794  |  0.372382  | 0.899188 |  0.100812  | 01:27  |
|   5   |  0.215737  |  0.319737  | 0.920839 |  0.079161  | 01:25  |
|   6   |  0.156200  |  0.319586  | 0.924899 |  0.075101  | 01:27  |
|   7   |  0.110415  |  0.235808  | 0.936401 |  0.063599  | 01:27  |
|   8   |  0.078930  |  0.260270  | 0.929635 |  0.070365  | 01:27  |
|   9   |  0.065367  |  0.257863  | 0.934371 |  0.065629  | 01:26  |

- **Model Training**: The training process unfolds across 10 epochs, allowing our model to progressively learn and adapt to the dataset. Throughout this journey, the model continually refines its understanding of the pet breed classification task.

- **Performance Metrics**: The model's performance is comprehensively assessed through essential metrics, encompassing both training and validation datasets. These metrics include:
  - **Training Loss**: The training loss progressively decreases from 0.667 in the initial epoch to 0.065 in the final epoch. This metric quantifies the error during model training and illustrates the model's improved accuracy in predicting pet breeds.
  - **Validation Loss**: The validation loss follows a similar trend, diminishing from 0.321 to 0.258. This metric assesses the model's generalization capability, showing its ability to make accurate predictions on unseen data.
  - **Accuracy**: The accuracy metric measures the proportion of correctly classified images. It escalates from 89.5% in the first epoch to an impressive 93.4% in the final epoch, demonstrating the model's proficiency in pet breed classification.
  - **Error Rate**: The error rate, inversely related to accuracy, exhibits a declining pattern. It decreases from 10.5% to a remarkable 6.6%, highlighting the model's increasingly precise categorization.

- **Time Tracking**: Each epoch's training duration is recorded, reflecting the computational efficiency of the model training process. The training time remains consistent, with each epoch taking approximately 1 minute and 27 seconds.

- **Model Preservation**: Upon successful completion of training, the code saves the trained model as 'model1_freezed.' This preserved model encapsulates not only the architecture but also the learned weights and capabilities, representing a significant achievement in our computer vision project.

This section serves as the crucible of our computer vision endeavor, where the model transforms from an initial state to a proficient classifier of pet breeds. The provided metrics underscore the model's remarkable progress, ultimately achieving impressive accuracy and precision. The saved model, 'model1_freezed,' becomes a valuable asset, ready for evaluation, deployment, and real-world applications.

### Model Interpretation
**Top 10 Metrics from Classification Report**

| Breed Category            | Precision | Recall | F1-Score |
|---------------------------|-----------|--------|----------|
| Abyssinian                | 0.86      | 0.93   | 0.89     |
| Bengal                    | 0.92      | 0.73   | 0.81     |
| Siamese                   | 0.90      | 1.00   | 0.95     |
| Birman                    | 0.90      | 0.92   | 0.91     |
| Bombay                    | 0.98      | 0.98   | 0.98     |
| British_Shorthair         | 0.94      | 0.80   | 0.86     |
| Ragdoll                   | 0.81      | 0.91   | 0.86     |
| Maine_Coon                | 0.90      | 0.90   | 0.90     |
| Persian                   | 0.97      | 0.85   | 0.91     |
| Russian_Blue              | 0.79      | 0.94   | 0.86     |

- **Breed Category**: This column lists the different pet breed categories that the model is trained to classify.

- **Precision**: Precision measures how many of the predicted positive instances are actually positive. For example, for the category "Abyssinian," the precision is 0.86, which means that when the model predicts an image as Abyssinian, it is correct 86% of the time.

- **Recall**: Recall measures how many of the actual positive instances were correctly predicted. For "Siamese," the recall is 1.00, indicating that the model correctly identifies all Siamese images.

- **F1-Score**: The F1-Score is the harmonic mean of precision and recall. It provides a balanced measure of a model's accuracy. For "Bengal," the F1-Score is 0.81, indicating a good balance between precision and recall.

**Top 10 Most Confused Categories**

| Category Pair                        | Confusion Count |
|-------------------------------------|-----------------|
| British_Shorthair vs. Russian_Blue  | 5               |
| Beagle vs. Basset_Hound             | 5               |
| Bengal vs. Abyssinian               | 4               |
| Persian vs. Ragdoll                 | 4               |
| Ragdoll vs. Birman                 | 4               |
| Chihuahua vs. Miniature_Pinscher    | 4               |
| Bengal vs. Maine_Coon               | 3               |
| Birman vs. Siamese                  | 3               |
| Maine_Coon vs. Ragdoll              | 3               |
| American_Pit_Bull_Terrier vs. Miniature_Pinscher | 3 |

- **Category Pair**: This column lists pairs of pet breed categories that the model frequently confuses.

- **Confusion Count**: The "Confusion Count" indicates how many times the model confused one category for another. For example, "British_Shorthair vs. Russian_Blue" was confused 5 times, suggesting that the model often struggled to distinguish between these two breeds.

- **Model Loading**: The code initiates by loading the previously saved 'model1_freezed,' enabling us to work with the trained model for interpretation.

- **Interpretation Initialization**: Utilizing FastAi's `ClassificationInterpretation`, we gain access to a powerful set of tools for model evaluation and understanding. This tool allows us to delve deep into the model's predictions and validation results.

- **Losses and Indices**: The code extracts two pivotal pieces of information:
  - **Losses**: These values represent the model's prediction errors for the validation dataset. High losses indicate instances where the model faced challenges in making accurate predictions.
  - **Indices**: These are the indices corresponding to the validation dataset where the top losses occurred, providing context for the erroneous predictions.

- **Consistency Check**: A consistency check confirms that the lengths of the validation dataset, losses, and indices are equal, ensuring the integrity of the interpretation process.

- **Classification Report**: The code generates a detailed classification report, presenting precision, recall, F1-score, and support for each pet breed category. Here are a few illustrative examples:

    - **Abyssinian**: Precision of 0.86, Recall of 0.93, F1-score of 0.89.
    - **Bengal**: Precision of 0.92, Recall of 0.73, F1-score of 0.81.
    - **Siamese**: Precision of 0.90, Recall of 1.00, F1-score of 0.95.

  This report offers a granular view of the model's classification performance, highlighting areas of strength and potential improvement.

- **Most Confused Categories**: The code identifies and reports the categories that the model finds most challenging to distinguish. It reveals the pairs of pet breeds where the model frequently confuses one for the other, providing valuable insights for further refinement. For instance:

    - **British_Shorthair** is often confused with **Russian_Blue**, with 5 occurrences.
    - **Beagle** and **Basset_Hound** are confused 5 times.
    - **Bengal** is sometimes mistaken for **Abyssinian**, with 4 instances.

### Conclusion
This project showcases our proficiency in deep learning, data preprocessing, model training, and evaluation. The pet classifier we've developed delivers impressive results, accurately identifying pet breeds from a challenging dataset. Our pipeline can be adapted for various image classification tasks, and the model's ability to handle complex, real-world data demonstrates its practical utility.

**Model Performance Metrics (After Fine-Tuning):**
- Accuracy: Exceeding 94%
- Error Rate: Remarkably low
